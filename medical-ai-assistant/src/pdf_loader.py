"""
Medical PDF Loader and Processor
"""

import PyPDF2
from pathlib import Path
from .nlp_preprocessor import MedicalTextPreprocessor

class MedicalPDFLoader:
    def __init__(self, knowledge_base_path="medical_knowledge"):
        self.knowledge_base = Path(knowledge_base_path)
        self.preprocessor = MedicalTextPreprocessor()
        self.documents = []  # [{text, metadata}, ...]
        
    def load_all_documents(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ PDF –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏"""
        if not self.knowledge_base.exists():
            print(f"‚ö†Ô∏è –ü–∞–ø–∫–∞ {self.knowledge_base} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return []
        
        print(f"üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑: {self.knowledge_base}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º PDF —Ñ–∞–π–ª—ã
        pdf_files = list(self.knowledge_base.glob("*.pdf"))
        for pdf_path in pdf_files:
            self._load_pdf(pdf_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        txt_files = list(self.knowledge_base.glob("*.txt"))
        for txt_path in txt_files:
            self._load_txt(txt_path)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}")
        return self.documents
    
    def _load_pdf(self, pdf_path):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF"""
        try:
            print(f"  üìÑ –ó–∞–≥—Ä—É–∂–∞—é PDF: {pdf_path.name}")
            text = ""
            
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                num_pages = len(pdf_reader.pages)
                
                for page_num in range(num_pages):
                    page = pdf_reader.pages[page_num]
                    page_text = page.extract_text()
                    
                    if page_text.strip():
                        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
                        processed = self.preprocessor.process(page_text)
                        text += processed['processed_text'] + "\n\n"
            
            if text.strip():
                self.documents.append({
                    'text': text.strip(),
                    'source': pdf_path.name,
                    'type': 'pdf',
                    'pages': num_pages,
                    'preprocessing_stats': {
                        'original_length': len(text),
                        'processed_length': len(text.strip())
                    }
                })
                
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {pdf_path.name}: {e}")
    
    def _load_txt(self, txt_path):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã"""
        try:
            print(f"  üìù –ó–∞–≥—Ä—É–∂–∞—é TXT: {txt_path.name}")
            
            with open(txt_path, 'r', encoding='utf-8') as file:
                raw_text = file.read()
                
                # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
                processed = self.preprocessor.process(raw_text)
                
                self.documents.append({
                    'text': processed['processed_text'],
                    'source': txt_path.name,
                    'type': 'txt',
                    'preprocessing_stats': {
                        'original_length': len(raw_text),
                        'processed_length': len(processed['processed_text']),
                        'tokens_count': len(processed['lemmatized'])
                    }
                })
                
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {txt_path.name}: {e}")
    
    def get_statistics(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
        if not self.documents:
            return {"total_documents": 0}
        
        total_text_length = sum(len(doc['text']) for doc in self.documents)
        pdf_count = sum(1 for doc in self.documents if doc['type'] == 'pdf')
        txt_count = sum(1 for doc in self.documents if doc['type'] == 'txt')
        
        return {
            'total_documents': len(self.documents),
            'pdf_files': pdf_count,
            'txt_files': txt_count,
            'total_text_characters': total_text_length,
            'avg_text_length': total_text_length // len(self.documents)
        }
    
    def print_sample(self, doc_index=0, max_chars=500):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ–±—Ä–∞–∑–µ—Ü —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if not self.documents:
            print("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return
        
        doc = self.documents[doc_index]
        print(f"\n{'='*60}")
        print(f"üìë –î–æ–∫—É–º–µ–Ω—Ç #{doc_index + 1}: {doc['source']} ({doc['type']})")
        print(f"{'='*60}")
        print("üìÑ –¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ {} —Å–∏–º–≤–æ–ª–æ–≤):".format(max_chars))
        print("-"*40)
        print(doc['text'][:max_chars] + "...")
        print("-"*40)
        print("üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:", doc.get('preprocessing_stats', {}))